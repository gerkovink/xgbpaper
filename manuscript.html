<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.551">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Huma Shehwana">
<meta name="author" content="Gerko Vink">
<meta name="dcterms.date" content="2024-06-28">
<meta name="keywords" content="XGboost, Multiple Imputation, mice">

<title>The XGBoost Paradigm for Missing Data: Is It Worth the Hype</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="manuscript_files/libs/clipboard/clipboard.min.js"></script>
<script src="manuscript_files/libs/quarto-html/quarto.js"></script>
<script src="manuscript_files/libs/quarto-html/popper.min.js"></script>
<script src="manuscript_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="manuscript_files/libs/quarto-html/anchor.min.js"></script>
<link href="manuscript_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="manuscript_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="manuscript_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="manuscript_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="manuscript_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="manuscript_files/libs/kePrint-0.0.1/kePrint.js"></script>
<link href="manuscript_files/libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<link href="manuscript_files/libs/tabwid-1.1.3/tabwid.css" rel="stylesheet">
<script src="manuscript_files/libs/tabwid-1.1.3/tabwid.js"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#implementing-xgboost-in-a-fully-conditional-specification-framework" id="toc-implementing-xgboost-in-a-fully-conditional-specification-framework" class="nav-link" data-scroll-target="#implementing-xgboost-in-a-fully-conditional-specification-framework">Implementing XGBoost in a fully conditional specification framework</a></li>
  <li><a href="#algorithm-i-mice-xgb" id="toc-algorithm-i-mice-xgb" class="nav-link" data-scroll-target="#algorithm-i-mice-xgb">Algorithm I: <code>mice-xgb</code></a>
  <ul class="collapse">
  <li><a href="#input" id="toc-input" class="nav-link" data-scroll-target="#input">Input</a></li>
  <li><a href="#initialization" id="toc-initialization" class="nav-link" data-scroll-target="#initialization">Initialization</a></li>
  <li><a href="#iteration-and-imputation-process" id="toc-iteration-and-imputation-process" class="nav-link" data-scroll-target="#iteration-and-imputation-process">Iteration and Imputation process</a></li>
  <li><a href="#output" id="toc-output" class="nav-link" data-scroll-target="#output">Output</a></li>
  </ul></li>
  <li><a href="#algorithm-ii-xgboost-hyperparameter-tuning-using-bayesian-optimization" id="toc-algorithm-ii-xgboost-hyperparameter-tuning-using-bayesian-optimization" class="nav-link" data-scroll-target="#algorithm-ii-xgboost-hyperparameter-tuning-using-bayesian-optimization">Algorithm II: XGBoost hyperparameter tuning using bayesian optimization</a>
  <ul class="collapse">
  <li><a href="#input-1" id="toc-input-1" class="nav-link" data-scroll-target="#input-1">Input</a></li>
  <li><a href="#training-features-and-label-variable-selection" id="toc-training-features-and-label-variable-selection" class="nav-link" data-scroll-target="#training-features-and-label-variable-selection">Training features and label variable selection</a>
  <ul class="collapse">
  <li><a href="#define-parameter-space" id="toc-define-parameter-space" class="nav-link" data-scroll-target="#define-parameter-space">Define parameter space</a></li>
  <li><a href="#initial-points-selection" id="toc-initial-points-selection" class="nav-link" data-scroll-target="#initial-points-selection">Initial Points Selection:</a></li>
  <li><a href="#k-fold-cross-validation-function" id="toc-k-fold-cross-validation-function" class="nav-link" data-scroll-target="#k-fold-cross-validation-function">K-fold Cross Validation function</a></li>
  </ul></li>
  <li><a href="#bayesian-optimization" id="toc-bayesian-optimization" class="nav-link" data-scroll-target="#bayesian-optimization">Bayesian Optimization</a></li>
  <li><a href="#output-1" id="toc-output-1" class="nav-link" data-scroll-target="#output-1">Output</a></li>
  </ul></li>
  <li><a href="#simulation-study" id="toc-simulation-study" class="nav-link" data-scroll-target="#simulation-study">Simulation study</a>
  <ul class="collapse">
  <li><a href="#aim" id="toc-aim" class="nav-link" data-scroll-target="#aim">Aim</a></li>
  <li><a href="#data-generation" id="toc-data-generation" class="nav-link" data-scroll-target="#data-generation">Data generation</a></li>
  </ul></li>
  <li><a href="#imputation-models-used-in-simulation-study" id="toc-imputation-models-used-in-simulation-study" class="nav-link" data-scroll-target="#imputation-models-used-in-simulation-study">Imputation models used in simulation study</a>
  <ul class="collapse">
  <li><a href="#estimand-and-evaluation-metrics" id="toc-estimand-and-evaluation-metrics" class="nav-link" data-scroll-target="#estimand-and-evaluation-metrics">Estimand and evaluation metrics</a></li>
  </ul></li>
  <li><a href="#hyperparameter-tuning-using-xgb_param_calc" id="toc-hyperparameter-tuning-using-xgb_param_calc" class="nav-link" data-scroll-target="#hyperparameter-tuning-using-xgb_param_calc">Hyperparameter tuning using <code>xgb_param_calc()</code></a>
  <ul class="collapse">
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a>
  <ul class="collapse">
  <li><a href="#hyperparameter-tuning" id="toc-hyperparameter-tuning" class="nav-link" data-scroll-target="#hyperparameter-tuning">Hyperparameter tuning</a></li>
  <li><a href="#computational-time-for-hyperparameter-tuning" id="toc-computational-time-for-hyperparameter-tuning" class="nav-link" data-scroll-target="#computational-time-for-hyperparameter-tuning">Computational time for hyperparameter tuning</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#simulation-results" id="toc-simulation-results" class="nav-link" data-scroll-target="#simulation-results">Simulation results</a>
  <ul class="collapse">
  <li><a href="#computational-time-required-for-imputation" id="toc-computational-time-required-for-imputation" class="nav-link" data-scroll-target="#computational-time-required-for-imputation">Computational time required for imputation</a></li>
  </ul></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion">Discussion</a></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">The XGBoost Paradigm for Missing Data: Is It Worth the Hype</h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Authors</div>
  <div class="quarto-title-meta-heading">Affiliations</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Huma Shehwana </p>
  </div>
  <div class="quarto-title-meta-contents">
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Gerko Vink <a href="mailto:g.vink@uu.nl" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0000-0001-9767-1924" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Utrecht University
          </p>
        <p class="affiliation">
            
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">June 28, 2024</p>
    </div>
  </div>
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="block-title">Abstract</div>
    <p>Tree-based imputation methods, such as Classification and Regression Trees (CART) and Random Forest (RF), have demonstrated improved accuracy in handling missing data, particularly when variables exhibit non-linear relationships. In our study, we developed another such method: XGBoost, implemented as an imputation model within the fully conditional specification framework of the <code>mice</code> package. Similar to mixgb, we integrated XGBoost predictions with donor-based matching using the observed data or predicted space of observed data. Our findings underscored that integrating XGBoost with donor-based selection yielded superior performance compared to using XGBoost predictions alone. Furthermore, we incorporated a Bayesian optimization-based parameter tuning function to improve the accuracy This function facilitates tuning parameters either for individual variables or separately for each variable in the dataset. Our comparative analysis indicated that while XGBoost demonstrated comparable performance to existing tree-based imputation methods under scenarios with low levels of missing data, CART and RF exhibited better accuracy and lower bias in scenarios with high levels of missing data. Moreover, XGBoost offers a diverse array of parameters, and optimizing these parameters is highly dataset-specific. Further research is necessary to ascertain whether specific parameter configurations can improve performance in contexts with high levels of missing data.</p>
  </div>
</div>

<div>
  <div class="keywords">
    <div class="block-title">Keywords</div>
    <p>XGboost, Multiple Imputation, mice</p>
  </div>
</div>

</header>


<section id="funding" class="level5">
<h5 class="anchored" data-anchor-id="funding">Funding</h5>
<p>This work has been funded by the Netherlands Organization for Scientific Research (NWO) under grant number 406.XS.01.104.</p>
</section>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Missing data is a prevalent issue across various fields and can potentially introduce bias into statistical analyses. To address this challenge, imputation has emerged as a widely utilized technique <span class="citation" data-cites="littleMissingDataAdjustmentsLarge1988">(<a href="#ref-littleMissingDataAdjustmentsLarge1988" role="doc-biblioref">Little 1988</a>)</span>. While single imputation may suffice for predicting values<span class="citation" data-cites="nijmanRealtimeImputationMissing2021">(<a href="#ref-nijmanRealtimeImputationMissing2021" role="doc-biblioref">Nijman et al. 2021</a>)</span>, the ultimate objective of analysis often lies in drawing meaningful inferences from the data. To accommodate this goal, Rubin proposed the multiple imputation strategy <span class="citation" data-cites="rubinInferenceMissingData1976">(<a href="#ref-rubinInferenceMissingData1976" role="doc-biblioref">RUBIN 1976</a>)</span>. This method entails replacing each missing value with a set of plausible substitute values. Within this framework, missing values within a variable are predicted using existing data of other variables. Subsequently, these predicted values are used to replace the missing ones, and the process is iteratively applied across all variables in the dataset, yielding a single imputed dataset. This procedure is then repeated to generate multiple imputed datasets. These datasets can be directly utilized in subsequent statistical analyses to derive parameters of scientific interest. Ultimately, estimates from all datasets are pooled into a single inference using Rubin’s rule, which integrates uncertainty among different imputed datasets<span class="citation" data-cites="Rubin1987 obermanStandardizedEvaluationImputation2024">(<a href="#ref-Rubin1987" role="doc-biblioref"><span>“Rubin, Page 76”</span> 1987</a>; <a href="#ref-obermanStandardizedEvaluationImputation2024" role="doc-biblioref">Oberman and Vink 2024</a>)</span>. The multiple imputation strategy has gained widespread acceptance due to its ability to restore natural variability and incorporate inherent uncertainty originating from missing data. By integrating information from correlated variables, multiple imputation preserves natural variability and addresses uncertainty by proposing various versions of imputed datasets. Variability between these imputed datasets can account for the imputation uncertainty. Moreover, this strategy has proven impactful in smaller datasets or those with a large number of missing values <span class="citation" data-cites="kangPreventionHandlingMissing2013">(<a href="#ref-kangPreventionHandlingMissing2013" role="doc-biblioref">Kang 2013</a>)</span>.</p>
<p>Initially, two methods, Joint Modeling and Fully Conditional Specification (FCS), were introduced for imputation <span class="citation" data-cites="vanbuurenFullyConditionalSpecification2006">(<a href="#ref-vanbuurenFullyConditionalSpecification2006" role="doc-biblioref">Van Buuren et al. 2006</a>)</span>. Joint modeling relies on the assumption of joint multivariate normality among all variables, which can be restrictive for real-life datasets that are highly heterogeneous <span class="citation" data-cites="schaferMissingDataOur2002">(<a href="#ref-schaferMissingDataOur2002" role="doc-biblioref">Schafer and Graham 2002</a>)</span>. In contrast, FCS operates by imputing data on a variable-by-variable basis, providing flexibility in selecting appropriate imputation methods for each variable based on its characteristics. FCS has gained popularity for its ability to produce unbiased estimates with adequate coverage <span class="citation" data-cites="vanbuurenFullyConditionalSpecification2006">(<a href="#ref-vanbuurenFullyConditionalSpecification2006" role="doc-biblioref">Van Buuren et al. 2006</a>)</span>. The quality of imputation results heavily depends on the statistical properties of the missing variable and the selection of an appropriate imputation model capable of capturing these properties. Moreover, Meng (1994) emphasized the importance of the imputation model being congenial to the downstream analysis model, highlighting the need for effective alignment between the two <span class="citation" data-cites="Rubin_1994">(<a href="#ref-Rubin_1994" role="doc-biblioref">Efron 1994</a>)</span>. Often, data analysis models include relationships between different variables in the dataset. To ensure seamless integration between these steps, the imputation model should not only be selected according to each variable type but must also be able to capture the complex relationships among different variables in the data. However, available software packages such as mi and <code>mice</code> may not adequately capture these complex relationships by default. Therefore, it is crucial to manually specify all interactions between variable in order to obtain statistically valid imputed datasets.</p>
<p>In recent years, there has been significant interest in using machine learning algorithms for imputing missing data, owing to their extensive application in prediction tasks. It is crucial to ensure compatibility by employing machine learning-based imputation models. Additionally, traditional imputation methods often struggle to capture non-linear relationships between variables, presenting a significant challenge . Tree-based methods have emerged as a solution to this issue. The initial integration of random forest-based imputation models into packages like missForest and <code>mice</code> (<code>mice-RF</code> and <code>mice-CART</code>) has demonstrated their effectiveness in preserving complex relationships between variables compared to standard methods <span class="citation" data-cites="stekhovenMissForestNonparametricMissing2012">(<a href="#ref-stekhovenMissForestNonparametricMissing2012" role="doc-biblioref">Stekhoven and Bühlmann 2012</a>)</span>. Another notable tree-based non-parametric method is XGBoost, specifically designed to capture intricate data relationships. In this context, we introduce XGBoost as a model for imputation within the framework of fully conditional specification. We anticipate that employing XGBoost for imputation will enhance accuracy, particularly in scenarios where traditional methods fail due to non-linear data relationships between variables.</p>
<p>The aim of this study is two fold. First, we introduce an imputation methodology based on the XGBoost algorithm, readily available in the <code>mice</code> package. Hyperparameter selection significantly impacts machine learning algorithm performance. We offer users flexible options to tune these hyperparameters: 1) tuning parameters on a single variable of interest and applying them to impute all variables in the dataset, 2) iteratively tuning hyperparameters using each variable as a target variable, or 3) utilizing default hyper parameters. We employ Bayesian optimization for hyperparameter selection as the technique is well-known for its efficiency in iteratively exploring the hyperparameter search space in an informed manner <span class="citation" data-cites="wulffMultipleImputationChained2017">(<a href="#ref-wulffMultipleImputationChained2017" role="doc-biblioref">Wulff and Ejlskov 2017</a>)</span>. Second, we present simulation results comparing the imputation performance of traditional models (predictive mean matching, random forest, classification and regression tree) and previously proposed XGBoost implementation (mixgb) package <span class="citation" data-cites="dengMultipleImputationXGBoost2023">(<a href="#ref-dengMultipleImputationXGBoost2023" role="doc-biblioref">Deng and Lumley 2023</a>)</span> with our newly developed <code>mice-xgb</code>oost method. Our results show that <code>mice-xgb</code>oost perform significantly better than all other methods specifically to capture quadratic relationship and interaction between variables.</p>
</section>
<section id="implementing-xgboost-in-a-fully-conditional-specification-framework" class="level1">
<h1>Implementing XGBoost in a fully conditional specification framework</h1>
<p>Recently, the <code>mixgb</code> package has extended the application of XGBoost beyond prediction, employing it as a strategy for missing data imputation. The <code>mixgb</code> package integrates the uncertainty and variability inherent in missing data by introducing two features. First, <code>mixgb</code> introduces a fusion of XGBoost with predictive mean matching (PMM) approach. PMM addresses the underestimation by finding donors of predicted values from observed data in a predicted space. Secondly, the <code>mixgb</code> package takes the benefit of sub-sampling parameter for training the XGBoost model. Moreover, <code>mixgb</code> is built on a non-iterative imputation framework while also allowing users to perform multiple iterations if needed. The MixGB workflow includes 1) sorting variables based on the missing proportion in each variable, 2) performing initial imputation (e.g., using mean, median or normal distribution), 3) using sub-sampling to train the XGBoost model, 4) making predictions, and 5) either returning the predicted value or identifying the best donor from the prediction space of observed data in the case of PMM. Deng et al.&nbsp;described that this approach effectively accounts for the variability of missing data and improves imputation performance compared to existing methods <span class="citation" data-cites="dengMultipleImputationXGBoost2023">(<a href="#ref-dengMultipleImputationXGBoost2023" role="doc-biblioref">Deng and Lumley 2023</a>)</span>.</p>
<p><code>mice</code> is an extensible package which is evident by subsequent addition of multiple tree-based imputation algorithms such as CART and Random Forest. This study extends the mission of integrating machine learning techniques with missing data imputation by specifically implementing XGBoost into the foundational framework of the <code>mice</code> package. Similar to the <code>mixgb</code> package, we have combined XGBoost predictions with donor-based selection (referred to as PMM in mixgb) to impute missing data. To accomplish this, one of the three “match.types” can be provided as an input option in mice.impute.xgb function: “predicted,” “predicted.observed,” and “original.observed”.</p>
<ul>
<li><p>The “predicted” match type uses missing data predictions directly obtained from the XGBoost algorithm, analogous to setting pmm = null in mixgb.</p></li>
<li><p>The “predicted.observed” and “original.observed” match types operate on the assumption that the distribution of missing data points is same as the distribution of observed data <span class="citation" data-cites="buurenFlexibleImputationMissing">(<a href="#ref-buurenFlexibleImputationMissing" role="doc-biblioref">Buuren, n.d.</a>)</span>.</p>
<ul>
<li><p>For the “predicted.observed” match type, the missing data predicted values obtained form XGBoost are matched to the prediction space of observed data points for the corresponding variable. A set of closely matched donor points is selected (d = 5), and one value is randomly chosen from these donors to replace the missing data point. This method is equivalent to setting pmm = 2 in mixgb.</p></li>
<li><p>Conversely, the “original.observed” match type involves matching the predicted value of a missing data point to the original data, rather than the prediction space of observed data in order to select donors. This approach differs from pmm = 1 in mixgb.</p></li>
</ul></li>
</ul>
<p>Another notable feature of <code>mice-xgb</code> is its customizability in handling hyperparameters for the XGBoost model. User can opt to run imputation task with default parameters or input preferred parameters based on prior experience / literature search. Moreover, if user wants to experiment with hyperparameters, we have provided a companion function, “xgb_param_calc”, which is designed to extract hyperparameters specifically optimized for a given dataset. A traditional prediction problems revolves around predicting one target variables using other available variables. On contrary, in the context of imputation, all variables are iteratively imputed in a cyclic manner thus each variable act as a target variable, making it a multi-task problem. Therefore, users must decide whether to use the same set of parameters for all the imputation tasks or to specify separate set of hyperparameters for each target variable. <code>mice</code> XGboost provides both such options. User have the flexibility to first tune a uniform set of parameters for imputing all variables or separate set of hyperparameters specific to each variable and in the next step, provide these parameters to XGBoost imputation task. xgb_param_calc function employs Bayesian optimization and cross-validation to identify the optimal set of parameters that resulted in minimum testing mean absolute percentage error (MAPE) out of all the tested set of parameters. Users can opt to tune any combination of nine most widely tuned parameters, leaving the rest of the parameters to their default value. These parameters and their range are listed in Table 1. xgb_pram_calc function can be run with one of the three options 1) randomly select one variable as the target for tuning parameters, 2) tune hyperparameters using a user-selected variable as the target, or 3) obtain separate parameter values for all variables. Third approach is useful for handling complex relationships within the data, though it is more time-consuming.</p>
<table class="table">
<caption>Table 1: Available hyperparameters in the <code>mice::xgb_param_calc()</code> function</caption>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Range</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Learning Rate - Eta</td>
<td>0 - 0.3</td>
</tr>
<tr class="even">
<td>Nrounds</td>
<td>100 - 1000</td>
</tr>
<tr class="odd">
<td>max_depth</td>
<td>2 - 12</td>
</tr>
<tr class="even">
<td>Min_child_weight</td>
<td>0 - 25</td>
</tr>
<tr class="odd">
<td>Subsample</td>
<td>0.5 - 1</td>
</tr>
<tr class="even">
<td>Colsample_bytree</td>
<td>0 - 25</td>
</tr>
<tr class="odd">
<td>Gamma</td>
<td>1e-09, 10</td>
</tr>
<tr class="even">
<td>alpha</td>
<td>1e-09, 100</td>
</tr>
<tr class="odd">
<td>lambda</td>
<td>1e-09, 100</td>
</tr>
</tbody>
</table>
<p>The basic work-flow of the <code>mice</code> package includes three steps: initialization, iterative imputation and pooling. For a dataset Y with p incomplete variables Y = (Y1, Y2, …. , Y<span class="math inline">\(_p\)</span>), Y<span class="math inline">\(_j\)</span> denote one of the incomplete variable with j = 1,2,3,….p. Variable Y<span class="math inline">\(_j\)</span> is split in observed (Y<span class="math inline">\(_j^{obs}\)</span>) and missing part (Y<span class="math inline">\(_j^{mis}\)</span>). Initialization process begins by replacing missing values Y<span class="math inline">\(_j^{mis}\)</span> with plausible estimates. These initial values are randomly drawn from a distribution specifically modeled for missing data in each variable Y<span class="math inline">\(_j\)</span>. By default, <code>mice</code>initiates five imputed datasets (m&gt;1). The hth imputed data sets is referred as Y<span class="math inline">\(^(h)\)</span> where h = 1, . . . , m. These imputed datasets are identical for observed data but differ in their imputed values. The variability among these imputations reflects the uncertainty associated with the plausible values of the missing data. In contrast, the initialization in <code>mixgb</code> generates identical initial complete datasets. However, <code>mixgb</code> utilizes the subsampling hyperparameter of the xgboost algorithm to incorporate uncertainty. Thus for each imputation, a different subset of data is used for imputation resulting in different predicted values, reflecting variability between imputations. However, the use of the subsampling parameter in <code>mice-xgb</code>oost is optional. User may or may not tune the subsampling parameter along with other parameters depending on the dataset and the subsequent statistical question.</p>
<p>The next step, iterative imputation is now easy as all datasets are complete. Any statistical model can be employed, as it would be in standard statistical analysis outside the context of missing data. By default, <code>mice</code>use five iterations thus iterative imputation starts with 5 datasets Y<span class="math inline">\(^m\)</span> where m = 1,2,…5. For each imputation, one iteration involves cycling through all variables (j = 1,…., p) to replace placeholder missing values with estimates (Q) obtained from the imputation model. In <code>mice-xgb</code>, these estimates are the predictions from the XGBoost model. For each variable (Y<span class="math inline">\(_j\)</span>), xgboost model is trained on the subset of data where target variable (Y<span class="math inline">\(_j\)</span>) is observed. These observed values of Y<span class="math inline">\(_j\)</span> act as a label (Y<span class="math inline">\(_j^{obs}\)</span>) and all other variables in this subset of data are considered independent variables Y<span class="math inline">\(_{−j}^{obs}\)</span> = (Y<span class="math inline">\(_1^{obs}\)</span>,…,Y<span class="math inline">\(_{j-1}^{obs}\)</span>, Y<span class="math inline">\(_{j+1}^{obs}\)</span>, . . . , Y<span class="math inline">\(_{p}^{obs}\)</span>). Once model is trained, the trained model is fit on the missing data of Y<span class="math inline">\(_{j-1}^{mis}\)</span>, Y<span class="math inline">\(_{j+1}^{mis}\)</span>, . . . , Y<span class="math inline">\(_{p}^{mis}\)</span>) to obtain the prediction of missing data points of Y<span class="math inline">\(_{j}^{mis}\)</span> which is denoted as <span class="math inline">\(\hat{Y}_j^{mis}\)</span>. These predicted values (Q) are used to replace the initial placeholder values. The process is repeated for all variables j= 1,2,…p.&nbsp;After one complete cycle, placeholders from all variables are replaced with estimates (Q). This process is repeated for 5-10 iterations to ensure sufficient model convergence. Additionally, this entire procedure is performed separately for all 5 imputations to get 5 different datasets Y<span class="math inline">\(^(1,2,...,5)\)</span>. This iterative process ensures that the imputed values are progressively refined. However, user can choose to run only one iteration. The final step in the imputation process is pooling the estimates from different imputations. The <em>pool</em> function in <code>mice</code>is implemented using Rubin’s rules <span class="citation" data-cites="rubinInferenceMissingData1976">(<a href="#ref-rubinInferenceMissingData1976" role="doc-biblioref">RUBIN 1976</a>)</span> to facilitate pooling process.</p>
<p>Deng et al.&nbsp;highlighted that directly predicting missing values using the XGBoost algorithm can lead to an underestimation of variability between different imputations. To address this, they proposed integrating XGBoost predictions with a Predictive Mean Matching (PMM) inspired donor-based selection. PMM matches each missing value’s predicted value with closely matched observed values from the same variable (Yj), referred to as candidate donors. One donor is then randomly selected to impute the missing value. In the default settings of <code>mice-xgb</code>, missing values are replaced with direct predictions from the XGBoost model, represented as <span class="math inline">\(\hat{Y}_j^{mis}\)</span> (match.type = “predicted”). However, if the user opts for alternative match.types such as “predicted.observed” or “original.observed,” the algorithm performs donor-based matching using the predicted values <span class="math inline">\(\hat{Y}_j^{mis}\)</span> as the recipient space. With “predicted.observed” match.type, donors are chosen from the predictions of the trained model on the observed data <span class="math inline">\(\hat{Y}_j^{obs}\)</span>, analogous to pmm=2 in <code>mixgb</code> (without subsampling). For “original.observed” match.type, the donor set is obtained by matching the predicted values <span class="math inline">\(\hat{Y}_j^{mis}\)</span> to the original observed data Y<span class="math inline">\(_j^{obs}\)</span>. Unlike <code>mixgb</code> , there is no equivalent option for pmm=1 in mice. The main distinction between <code>mixgb</code>’s pmm=1 and pmm=2 lies in the source of <span class="math inline">\(\hat{Y}_j^{obs}\)</span>: pmm=2 uses a subsample of the dataset, while pmm=1 trains on the complete dataset. However, with no subsampling and a single iteration, <code>mixgb</code> ’s pmm=1 and pmm=2 yield the same donor space. Since <code>mice-xgb</code>’s default setting employs the complete dataset for training and prediction, a separate method for pmm=1 is unnecessary.</p>
</section>
<section id="algorithm-i-mice-xgb" class="level1">
<h1>Algorithm I: <code>mice-xgb</code></h1>
<p><em>Missing data imputation using <code>mice-xgb</code> with default parameters</em></p>
<section id="input" class="level2">
<h2 class="anchored" data-anchor-id="input">Input</h2>
<p>Input of <code>mice</code>for xgboost algorithm consist of following options.</p>
<ul>
<li><p>A dataset is represented by Y<span class="math inline">\(_{raw}\)</span> with j=1,2,… p variables.</p></li>
<li><p>Number of Imputation (m = 5)</p></li>
<li><p>Number of Iteration (maxit = 5)</p></li>
<li><p>Imputation method (method = “xgb”)</p></li>
<li><p>match.type = “predicted” (default); available options: “predicted”, “predicted.observed”, “original.observed”</p></li>
<li><p>params = NULL (default option); available options: list of parameter (described in table 1); a list of list with variable names and set of parameter for each variable</p></li>
</ul>
</section>
<section id="initialization" class="level2">
<h2 class="anchored" data-anchor-id="initialization">Initialization</h2>
<p>The basic workflow of <code>mice</code>starts with identifying missing values in the data Y and storing their location in w matrix. For each variable, missing values in Y<span class="math inline">\(_{raw}\)</span> is replaced by placeholder values obtained from distribution of each variable</p>
<p>For <span class="math inline">\(m = 1:5\)</span>:</p>
<p><span class="math display">\[ Y_{w,j}^m \leftarrow Y_{init} \]</span></p>
</section>
<section id="iteration-and-imputation-process" class="level2">
<h2 class="anchored" data-anchor-id="iteration-and-imputation-process">Iteration and Imputation process</h2>
<p>For m = 1:5</p>
<p>For maxit = 1:5</p>
<p>For each missing variable j, Y is split in three different subsets to impute missing value (denoted below)</p>
<ul>
<li><p>Y<span class="math inline">\(_j^{obs}\)</span> ~ represents the observed data for j variable</p></li>
<li><p>Y<span class="math inline">\(_{-j}^{obs}\)</span> ~ represents the data for all variables except j, where Y<span class="math inline">\(_j\)</span> was observed.</p></li>
<li><p>Y<span class="math inline">\(_{-j}^{mis}\)</span> ~ represents the data for all variables except j, where j was missing.</p></li>
<li><p>Y<span class="math inline">\(_j^{mis}\)</span> ~ will be predicted using Xgboost algorithm and user selected match type.</p></li>
</ul>
<p>First, the XGBoost model is trained on the data Y<span class="math inline">\(_\text{-j}^\text{obs}\)</span>.</p>
<p><span class="math display">\[
\hat{Y}_{j}^{obs} = \text{XGBoost}(Y_{-j}^{obs})
\]</span></p>
<p>Trained model (denoted as <span class="math inline">\(\hat{Y}_{obs}^{j}\)</span> is used to get the predictions for missing values in Y<span class="math inline">\(^j\)</span>:</p>
<p><span class="math display">\[
\hat{Y}_{\text{j}}^{mis} = \text{XGBoost}_{\hat{Y}_{\text{j}}^{obs}}(Y_{\text{j}}^{mis})
\]</span> After the initial prediction, algorithm proceed in one of the three direction</p>
<p>If match.type = “predicted” <span class="math display">\[
{Y}_{\text{j}}^{mis[m]} = \hat{Y}_{\text{j}}^{mis}
\]</span> else if match.type = “predicted.observed”</p>
<p><span class="math display">\[
    Y_{\text{j}}^{\text{mis[m]}} = \text{Donor value by matching } \hat{Y}_{\text{j}}^{\text{mis}} \text{ with } \hat{Y}_{\text{j}}^{\text{obs}}
  \]</span></p>
<p>else if match.type = “original.observed”</p>
<p><span class="math display">\[
  Y_{\text{j}}^{\text{mis[m]}} = \text{ Donor value by matching } \hat{Y}_{\text{j}}^{mis}  \text{ with } \text{Y}_{j}^{\text{obs}}
  \]</span></p>
</section>
<section id="output" class="level2">
<h2 class="anchored" data-anchor-id="output">Output</h2>
<p>Imputed datasets Y<span class="math inline">\(^{imp}\)</span> where <span class="math inline">\(\text{imp} = \{1, \ldots, m\}\)</span></p>
</section>
</section>
<section id="algorithm-ii-xgboost-hyperparameter-tuning-using-bayesian-optimization" class="level1">
<h1>Algorithm II: XGBoost hyperparameter tuning using bayesian optimization</h1>
<p>In <code>mice</code> package, hyperparameter tuning can be performed using xgb_param_calc function</p>
<section id="input-1" class="level2">
<h2 class="anchored" data-anchor-id="input-1">Input</h2>
<ul>
<li><p>A dataset (Y<span class="math inline">\(_{raw}\)</span>) with <span class="math inline">\(\text{j} = \{1, \ldots, p\}\)</span> variables</p></li>
<li><p>response = index of target variable j</p>
<p><em>other available options: “all”: separately tune prameters for all variables “null”: randomly select one target variable and tune parameters</em></p></li>
<li><p>select_features = index of variables used as a feature for training</p>
<p><em>other available options: “all” or “null”: Use all parameters except target variable as feature for training</em></p></li>
<li><p>Number of bayesian iterations (iter)</p></li>
<li><p>fixed_param: Parameters and their values (if user want to fix any value other than default value and not tune these parameters)</p></li>
</ul>
<p><span class="math display">\[
p_{\text{f}} = \{ p_{\text{f1}}, p_{\text{f2}}, \ldots, p_{\text{fN}} \}
\]</span></p>
<ul>
<li>Params_to_tune: name of parameters (from table 1) to be tuned.</li>
</ul>
<p><span class="math display">\[
p_{\text{t}} = \{ p_{\text{t1}}, p_{\text{t2}}, \ldots, p_{\text{tN}} \}
\]</span></p>
</section>
<section id="training-features-and-label-variable-selection" class="level2">
<h2 class="anchored" data-anchor-id="training-features-and-label-variable-selection">Training features and label variable selection</h2>
<p>Y<span class="math inline">\(_\text{raw}\)</span> &lt;- complete cases of Y<span class="math inline">\(_\text{raw}\)</span></p>
<p>If <code>response = NULL</code>:</p>
<ul>
<li><p>If <code>select_features = NULL</code> or <code>select_features = "all"</code>:</p>
<p><span class="math display">\[
Y_r = Y_{\text{j}} \leftarrow \text{Represents data of a randomly selected variable from } j \in 1, \ldots, p
\]</span></p>
<p><span class="math display">\[
Y_{-r} = Y_{-\text{j}} \leftarrow \text{Represents data of all variables except } Y_j
\]</span></p></li>
<li><p>if <code>select_features =  j+1, j+2</code></p>
<p><span class="math display">\[
  Y_r \leftarrow \text{represents data of randomly selected one variable from } \{ Y_j \mid j \in \{1, \ldots, p\} \setminus \{j+1, j+2\} \}
  \]</span></p>
<p><span class="math display">\[
  Y_{-r} \leftarrow \{Y_{j+1}, Y_{j+2} \}
  \]</span></p></li>
</ul>
<p>if <code>response = j</code>:</p>
<ul>
<li><p>if <code>select_features = NULL</code> or <code>select_features = "all"</code> <span class="math display">\[
Y_r \leftarrow Y_j
\]</span></p>
<p><span class="math display">\[
Y_{-r} \leftarrow Y_{-j} \text{ represents data of all variables except } Y_j
\]</span></p></li>
<li><p>if <code>select_features = j+1, j+2</code></p>
<p><span class="math display">\[
Y_r \leftarrow Y_j
\]</span></p>
<p><span class="math display">\[
Y_{-r} \leftarrow \{ Y_{j+1},  Y_{j+2} \}
\]</span></p></li>
</ul>
<p>if <code>response = "all"</code></p>
<p>For <span class="math inline">\(j = 1, \ldots, p\)</span>:</p>
<p><span class="math display">\[
Y_r \leftarrow Y_j
\]</span></p>
<p><span class="math display">\[
Y_{-r} \leftarrow Y_{-j} \text{ represents data of all variables except } Y_j
\]</span></p>
<section id="define-parameter-space" class="level3">
<h3 class="anchored" data-anchor-id="define-parameter-space">Define parameter space</h3>
<p>The list of parameters available in the tuning function are listed in Table 1.</p>
<p>Let <span class="math inline">\(\text{P}\)</span> represent the complete list of parameters <span class="math inline">\(P = \{p_1, p_2, \ldots, p_N\}\)</span>.</p>
<p>Redefine the parameter space <span class="math inline">\(\text{P}_q\)</span> by matching names of <span class="math inline">\(\text{P$_t$}\)</span> (Parameters to tune) with <span class="math inline">\(\text{P}\)</span> such that <span class="math inline">\(\text{P$_q$}\)</span> is a subset of <span class="math inline">\(\text{P}\)</span>:</p>
<p><span class="math display">\[
\text{P}_q \leftarrow \{ p_1, p_2, \ldots, p_N \} \subseteq \text{P}
\]</span></p>
</section>
<section id="initial-points-selection" class="level3">
<h3 class="anchored" data-anchor-id="initial-points-selection">Initial Points Selection:</h3>
<p>Randomly select 𝑛× 5 initial points for evaluation where <span class="math inline">\(n\)</span> represents the number of tuning parameters <span class="math inline">\(\text{P}_q\)</span></p>
</section>
<section id="k-fold-cross-validation-function" class="level3">
<h3 class="anchored" data-anchor-id="k-fold-cross-validation-function">K-fold Cross Validation function</h3>
<p>Define K-fold cross-validation based objective function as follow</p>
<p>Split the Y dataset into <span class="math inline">\(K\)</span> folds for cross-validation.</p>
<p><span class="math display">\[
\{(Y_{-r}^{(1)}, Y_r^{(1)}), (Y_{-r}^{(2)}, Y_r^{(2)}), \ldots, (Y_{-r}^{(K)}, Y_r^{(K)})\}
\]</span></p>
<p>where each <span class="math inline">\((Y_{-r}^{(k)}, Y_r^{(k)})\)</span> for <span class="math inline">\(k = 1, 2, \ldots, K\)</span> represents a fold with <span class="math inline">\(Y_{-r}^{(k)}\)</span> as the training data and <span class="math inline">\(Y_r^{(k)}\)</span> as the label.</p>
</section>
</section>
<section id="bayesian-optimization" class="level2">
<h2 class="anchored" data-anchor-id="bayesian-optimization">Bayesian Optimization</h2>
<ol type="1">
<li><p><strong>Initialization:</strong> Evaluate the objective function <span class="math inline">\(f(\mathbf{x})\)</span> for generated (n*5) initial points</p></li>
<li><p><strong>Surrogate modeling:</strong> Fit a surrogate function <span class="math inline">\(\hat{f}(\mathbf{x})\)</span> to the initial observed data points.</p></li>
<li><p><strong>Acquisition function:</strong> Acquisition function uses surrogate function to propose next set of parameters <span class="math inline">\((x_{next})\)</span> by balancing exploration versus exploitation. In this study, Expected improvement is used as a criteria for acquisition function.</p></li>
<li><p><strong>Evaluation:</strong> Evaluate next set of parameters <span class="math inline">\((x_{next})\)</span> using objective function.</p></li>
<li><p><strong>Update:</strong> Update the surrogate function <span class="math inline">\(\hat{f}(\mathbf{x})\)</span> with the new data point <span class="math inline">\((x_{next},𝑓(x_{next}))\)</span>.</p></li>
<li><p><strong>Iteration:</strong> Repeat step 3-5 until algorithm converges or maximum number of iteration has reached.</p></li>
</ol>
</section>
<section id="output-1" class="level2">
<h2 class="anchored" data-anchor-id="output-1">Output</h2>
<p>Hyperparameter values that returned minimum value of loss function (Mean Absolute Percentage Error).</p>
</section>
</section>
<section id="simulation-study" class="level1">
<h1>Simulation study</h1>
<section id="aim" class="level2">
<h2 class="anchored" data-anchor-id="aim">Aim</h2>
<p>The aim of this simulation study is to assess the efficacy of XBGoost in imputing missing data, particularly for capturing non-linear relationships. This study aims to address the following research questions:</p>
<ul>
<li>How does the XGBoost model perform in predicting missing data compared to that of existing state-of-the-art imputation models?</li>
<li>Are there discernible differences between predictions directly obtained from the XGBoost model and combining XGBoost prediction with donor based selection?</li>
<li>Does parameter tuning enhance the predictive performance of the XGBoost imputation model?</li>
<li>Are there any differences between iterative vs non-iterative XGBoost missing data imputation?</li>
<li>Are there any differences in simulation time of the XGBoost model compare to that of state-of-the-art models?</li>
</ul>
</section>
<section id="data-generation" class="level2">
<h2 class="anchored" data-anchor-id="data-generation">Data generation</h2>
<p>In order to answer the above mentioned questions, we generated dataset of sample size (n = 1000) with two continuously predictor variable (X and Z) drawn form a bivariate normal distribution with <span class="math inline">\(\mu = [4, 1]\)</span> respectively and covariance <span class="math inline">\(\Sigma\)</span>.</p>
<p><span class="math display">\[
\Sigma = \begin{pmatrix}
1 &amp; 0.7 \\
0.7 &amp; 1
\end{pmatrix}
\]</span></p>
<p>Outcome variable Y is continuous and is defined in the following model.</p>
<p><span class="math display">\[
y = 3x + z + z^2 + 3xz + \epsilon
\]</span></p>
<p><span class="math inline">\(\epsilon\)</span> represents the random noise derived from a normal distribution with <span class="math inline">\(\mu\)</span> of 0 and <span class="math inline">\(\sigma\)</span> of 20. 100 simulated datasets were generated using the same model.</p>
<p>Next, for each dataset, we introduced varying proportions of missingness (20%, 40%, 60%, and 80%) using the Missing At Random (MAR) mechanism, specifically employing a right-tailed approach. This process resulted in a complete missing data cohort comprising 4 sets of 100 datasets each, totaling 400 datasets. The complete set of 400 datasets were utilized in a comparative simulation study. However, a subset of 10 datasets from the 20% and 60% missingness mechanisms each, was selected to evaluate the number of iterations and to determine the optimal hyperparameters for tuning. All instances of missingness in this study were generated using the ampute function in the <code>mice</code> package.</p>
</section>
</section>
<section id="imputation-models-used-in-simulation-study" class="level1">
<h1>Imputation models used in simulation study</h1>
<p>We conducted a comparative analysis of the <code>mice-xgb</code> imputation method against several state-of-the-art imputation techniques. These methods included the default settings of <code>mice</code> (Predictive Mean Matching), tree-based imputation models from <code>mice</code> namely, <code>mice-RF</code> (Random Forest) and <code>mice-CART</code> (Classification and Regression Trees) — and the XGBoost implementation from <code>mixgb</code> . All <code>mice</code> methods were run with the default settings of five imputations (m = 5). The default settings for the <code>mixgb</code> algorithm included five imputations (m = 5), one iteration (maxit = 1), and a subsampling approach to ensure variability between imputations. Both <code>mice</code> and <code>mixgb</code> were run with iteration = 1 and iteration = 5 to compare the effect of iteration. We also evaluated the impact of predictive mean matching of <code>mixgb</code> by running imputation model for all three setting (pmm = NULL, pmm = 1, and pmm = 2) and for two iteration settings (maxit = 1, maxit = 5).</p>
<p>The <code>mice-xgb</code> method was also run with the default settings of five imputations (m = 5) using all three implemented match.type settings: “predicted”, “predicted.observed”, and “original.observed”. <code>mice-xgb</code> was first run with default parameters. Additionally, the impact of hyperparameter tuning was investigated. To achieve this, we run the <code>mice-xgb</code> model with either default parameters or the tuned parameters obtained from the newly implemented xgb_param_calc function in the mice package.</p>
<p>The xgb_param_calc function was run with two input conditions:</p>
<p>“response = NULL” : One variable was randomly selected as the target variable, and the remaining variables were treated as independent variables. Algorithm returned a list of best performing values for the hyperparameters.</p>
<p>“response = all”: This option runs the hyperparameter tuning algorithm iteratively for all three variables and returns three seperate list of optimal hyperparameter values identified for each variable. These tuned hyperparameters were then input to the <code>mice-xgb</code> algorithm to evaluate their impact on imputation performance.</p>
<section id="estimand-and-evaluation-metrics" class="level2">
<h2 class="anchored" data-anchor-id="estimand-and-evaluation-metrics">Estimand and evaluation metrics</h2>
<p>A Regression model <span class="math inline">\(y \sim 3x + z + z^2 + 3xz\)</span> is applied to imputed datasets obtained from each imputation method. Estimates are extracted <span class="math inline">\(\hat{E}\)</span> from the regression results and are compared with true estimates. The true estimates (E) are acquired by fitting the regression model to the complete dataset before generating missingness. The corresponding confidence intervals are denoted by CI.low and CI.high The accuracy of each imputation model is evaluated by computing the bias, coverage and width of the regression estimates.</p>
<p>Bias is defined as difference between true estimate and calculated estimate from imputed dataset (<span class="math inline">\(\hat{E}\)</span> - E). Validity of imputation models is estimated by the coverage.</p>
<p>Coverage is defined as 1 if true estimate lies within the range of true confidence Interval, otherwise 0 i.e.&nbsp;<span class="math inline">\(\hat{CI.low}\)</span> &lt; E &lt; <span class="math inline">\(\hat{CI.high}\)</span>.</p>
<p>Width is defined as <span class="math inline">\(\hat{CI.high}\)</span> - <span class="math inline">\(\hat{CI.low}\)</span>. All three evaluation metrics were averaged over number of simulations (100) to get a single value result.</p>
</section>
</section>
<section id="hyperparameter-tuning-using-xgb_param_calc" class="level1">
<h1>Hyperparameter tuning using <code>xgb_param_calc()</code></h1>
<p>A subset of data containing 10 simulated datasets from each of the 20% and 60% missing data cohorts was selected. The xgb_param_calc function allows users to tune nine widely used booster parameters, with the list and range of parameter values detailed in Table 1. In this study, multiple configurations were used to identify the optimal set of parameters. Hyperparameter tuning was initially performed with varying numbers of iterations (iter = 50, 75, 100) to evaluate the convergence of the algorithm. The algorithm was run with the input option response = “all”, which sequentially treats each variable in the dataset as the target variable and returns a list of parameters for all variables.</p>
<p>Identifying which parameters need to be tuned and which should be left to their default values is always a challenge. In XGBoost, it is recommended to start with eta and learning_rate, and if needed, proceed to tune tree-based parameters next, followed by regularization parameters. Based on this approach, we categorized the boosting parameters into three main groups.</p>
<ul>
<li>Group 1: learning rate and eta.</li>
<li>Group 2: tree based parameter including subsample, max_depth and min_child_weight.</li>
<li>Group 3: regularization parameters including alpha, lambda and gamma.</li>
</ul>
<p>We followed two approaches to find the optimal set of parameters</p>
<ol type="1">
<li><p>We sequentially included parameters in the tuning process. The xgb_param_calc function allows users to tune a specified set of parameters while keeping the rest at their default settings. Additionally, users can fix the value of certain parameters (other than the default value) and tune others. In the first cycle, we tuned the parameters for Group 1, i.e., “eta” and “nrounds,” without fixing any parameters, implying that all other parameters were left at their default values. In the subsequent iteration, we fixed “eta” and “nrounds” to their optimal values identified in the first cycle and tuned the Group 2 parameters by setting param_list = c(“max_depth”, “min_child_weight”, “subsample”) in the xgb_param_calc function. In the third iteration, we fixed the values obtained for both Group 1 and Group 2 parameters and tuned the Group 3 parameters by setting param_list = c(“gamma”, “alpha”, “lambda”). After tuning all the parameters, we re-tuned “eta” and “nrounds,” this time fixing all other parameters (Group 4).</p></li>
<li><p>Instead of sequentially adding parameters, we combined parameters from the groups described above into three possible combinations:</p></li>
</ol>
<ul>
<li>G1 + G2 parameters together: We tuned “eta” and “nrounds” along with “max_depth,” “min_child_weight,” and “subsample.”</li>
<li>G1 + G3 parameters together: We tuned “eta” and “nrounds” along with “gamma,” “alpha,” and “lambda.”</li>
<li>G1 + G2 + G3 parameters together: We tuned all the parameters from Group 1, Group 2, and Group 3, namely “eta,” “nrounds,” “max_depth,” “min_child_weight,” “subsample,” “gamma,” “alpha,” and “lambda.”</li>
</ul>
<p>This approach allowed for comprehensive tuning of parameters to identify the optimal set for the XGBoost model. Once the optimal parameters were identified for each of the settings, we evaluated their impact by running the imputation model (mice.impute.xgb). The model was then further evaluated using a regression model, and evaluation metrics including bias, coverage, and width were calculated.</p>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results</h2>
<section id="hyperparameter-tuning" class="level3">
<h3 class="anchored" data-anchor-id="hyperparameter-tuning">Hyperparameter tuning</h3>
<p>We first evaluated the number of iterations needed for better convergence of the Bayesian optimization model. The iteration plot showed that increasing the number of iterations did not change the pattern in the trend, suggesting that 50 iterations might be sufficient (Supplementary Data 1). Parameter tuning is a challanging task as the study involves 100 datasets and three different variables. To ensure convergence for all variables, it is important to note that increasing the number of iterations increases the run-time. Therefore, it is essential to determine the balance between the available computational resources and the improvement in performance.</p>
<p>XGBoost has many hyperparameters, and proper tuning of these parameters can significantly influence the performance of the boosting algorithm. Tuning of all the parameters is a complex and time-consuming task. In this section, we aim to identify the subset of parameters that is most relevant to the missing data imputation problem addressed in this paper. Hyperparameter tuning results are presented in Figure 1. We have plotted mean bias levels produced by each imputation method, with width used as a confidence interval around the bias estimates. Complete results are summarized in Table 2.</p>
<p>For 20% missingness, our analysis highlighted substantial improvements in bias reduction across various terms when employing parameter tuning models compared to default settings. Notably, all tuning approaches effectively minimized bias for the intercept, <span class="math inline">\(x\)</span>, and <span class="math inline">\(x:z\)</span> interaction terms, underscoring their utility in enhancing accuracy. For the <span class="math inline">\(z\)</span> term, results varied among models, with the G1, sequential tuning of G4, and G1_G2_together models proving most effective, significantly reducing bias compared to defaults. Sequential tuning of the G2 parameter notably reduced bias for the quadratic term <span class="math inline">\(z^2\)</span>. Next, we evaluated the width of these modesl. Width estimates of G1 and sequential tuning of G2 model generally mirrored width of default parameters, while sequential tuning of G3 and G1_G3_together showed increased width. In terms of coverage, we observed that running <code>mice-xgb</code> model with default parameters maintained high coverage across all terms and coverage also remained robust in parameter-tuned scenarios, except for slight reduction in coverage with G1_G3_together for the <span class="math inline">\(z\)</span> term. These findings underscore the importance of parameter tuning in improving estimation accuracy and highlight trade-offs in bias, width, and coverage that researchers should consider when selecting tuning strategies.</p>
<p>For dataset cohort with 60% missingness, parameter tuning results were highly variable (Fig 2). <code>mice-xgb</code> with default parameter resulted in very small bias for <span class="math inline">\(z\)</span> , <span class="math inline">\(Z^2\)</span> and interaction term <span class="math inline">\(x:z\)</span> and tuning parameter either increased the bias or even if there was slight decrease in bias observed in sequential tuning of G3 and G4, width of the bias estimate increased. However, we observed that width of G1_G2_together was either similar (minimal increase) or reduced for <span class="math inline">\(z\)</span>, <span class="math inline">\(Z^2\)</span> and <span class="math inline">\(x:z\)</span> with similar or improved coverage. Moreover, We observed that all parameter tuning groups except G1_G2_together decreased the bias for both intercept and <span class="math inline">\(x\)</span> term as compared to running <code>mice-xgb</code> algorithm with default parameter. Although we observed increase in bias in G1_G2_together and all_together, width of both groups was similar or slightly reduced as compared to default parameter and other parameter tuning groups. All other tuning methods resulted in increased width compared to default parameter (Table 2).</p>
<p>The dataset cohort with 60% missing data exhibited substantial variability in parameter tuning outcomes (Fig 2). <code>mice-xgb</code> model with default parameter demonstrated minimal bias for <span class="math inline">\(z\)</span>, <span class="math inline">\(Z^2\)</span>, and the interaction term <span class="math inline">\(x:z\)</span>. Tuning parameters generally increased bias for these terms, and even in cases where sequential tuning of G3 and G4 resulted in slight decreases in bias, the width of the bias estimate expanded. In contrast, the G1_G2_together approach either maintained similar widths or reduced them for <span class="math inline">\(z\)</span>, <span class="math inline">\(Z^2\)</span>, and <span class="math inline">\(x:z\)</span>, while achieving comparable or improved coverage. For other two terms intercept and <span class="math inline">\(x\)</span>, all parameter tuning groups except G1_G2_together resulted in reduced bias compared to the <code>mice-xgb</code> model with default parameters. Despite observing increased bias with G1_G2_together and all_together approaches, the width of both groups remained similar or slightly reduced compared to defaults and other tuning methods. Conversely, although all other tuning groups exhibited reduced bias, this improvement was accompanied with increased width compared to default parameters.</p>
<p>Our study on a dataset with 20% missing data demonstrated that improving imputation accuracy does not necessarily require tuning a large number of parameters. Additionally, we observed distinct outcomes between sequentially tuning groups versus tuning parameters together. As the percentage of missing data increases, we found that the parameter tuning process becomes increasingly intricate. Therefore, a nuanced and dataset-specific approach to parameter tuning is essential. Generalizing parameter settings across diverse datasets with 60% missing data proves challenging due to inherent variations in dataset characteristics and complexities.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="FigsAndTables/BiasPlotOfDifferentMethods_1.png" class="img-fluid figure-img" width="2250"></p>
<figcaption>Figure 1: Simulation results for hyperparameter tuning - 20% missingness: The simulation included 10 datasets, each comprising 1000 samples. <code>mice-xgb</code> was executed using default parameters for comparative analysis. Bias is represented on the x-axis, with each parameter tuning group depicted on the y-axis. A vertical line at 0 indicates optimal bias proximity to zero.</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="FigsAndTables/BiasPlotOfDifferentMethods_2.png" class="img-fluid figure-img" width="4500"></p>
<figcaption>Figure 2: Simulation results for hyperparameter tuning - 60% missingness: The simulation included 10 datasets, each comprising 1000 samples. <code>mice-xgb</code> was executed using default parameters for comparative analysis. Bias is represented on the x-axis, with each parameter tuning group depicted on the y-axis. A vertical line at 0 indicates optimal bias proximity to zero.</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div>
<table class="table table-hover table-sm table-striped small" data-quarto-postprocess="true">
<colgroup>
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
</colgroup>
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th" style="text-align: left; empty-cells: hide; border-bottom: hidden;"></th>
<th data-quarto-table-cell-role="th" style="text-align: left; empty-cells: hide; border-bottom: hidden;"></th>
<th colspan="5" data-quarto-table-cell-role="th" style="text-align: center; border-bottom: hidden; padding-bottom: 0; padding-left: 3px; padding-right: 3px;"><div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
20% missingness mechanism
</div></th>
<th colspan="5" data-quarto-table-cell-role="th" style="text-align: center; border-bottom: hidden; padding-bottom: 0; padding-left: 3px; padding-right: 3px;"><div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
60% missingness mechanism
</div></th>
</tr>
<tr class="odd">
<th style="text-align: left;" data-quarto-table-cell-role="th">method</th>
<th style="text-align: left;" data-quarto-table-cell-role="th">term</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">Estimate_20</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">true_vals_20</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">cov_20</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">bias_20</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">width_20</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">Estimate_60</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">true_vals_60</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">cov_60</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">bias_60</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">width_60</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">G1</td>
<td style="text-align: left;">x</td>
<td style="text-align: right;">3.104</td>
<td style="text-align: right;">3.071</td>
<td style="text-align: right;">0.8</td>
<td style="text-align: right;">0.033</td>
<td style="text-align: right;">6.594</td>
<td style="text-align: right;">2.833</td>
<td style="text-align: right;">3.071</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">-0.238</td>
<td style="text-align: right;">5.297</td>
</tr>
<tr class="even">
<td style="text-align: left;">G1</td>
<td style="text-align: left;">z</td>
<td style="text-align: right;">-0.664</td>
<td style="text-align: right;">1.017</td>
<td style="text-align: right;">0.9</td>
<td style="text-align: right;">-1.681</td>
<td style="text-align: right;">19.376</td>
<td style="text-align: right;">0.926</td>
<td style="text-align: right;">1.017</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">-0.091</td>
<td style="text-align: right;">12.024</td>
</tr>
<tr class="odd">
<td style="text-align: left;">G1</td>
<td style="text-align: left;">I(z^2)</td>
<td style="text-align: right;">0.406</td>
<td style="text-align: right;">0.848</td>
<td style="text-align: right;">0.9</td>
<td style="text-align: right;">-0.442</td>
<td style="text-align: right;">5.962</td>
<td style="text-align: right;">0.639</td>
<td style="text-align: right;">0.848</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">-0.209</td>
<td style="text-align: right;">3.315</td>
</tr>
<tr class="even">
<td style="text-align: left;">G1</td>
<td style="text-align: left;">x:z</td>
<td style="text-align: right;">3.592</td>
<td style="text-align: right;">3.046</td>
<td style="text-align: right;">0.9</td>
<td style="text-align: right;">0.546</td>
<td style="text-align: right;">6.434</td>
<td style="text-align: right;">3.215</td>
<td style="text-align: right;">3.046</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.169</td>
<td style="text-align: right;">4.048</td>
</tr>
<tr class="odd">
<td style="text-align: left;">G2</td>
<td style="text-align: left;">(Intercept)</td>
<td style="text-align: right;">-0.806</td>
<td style="text-align: right;">-0.114</td>
<td style="text-align: right;">0.9</td>
<td style="text-align: right;">-0.692</td>
<td style="text-align: right;">23.711</td>
<td style="text-align: right;">0.679</td>
<td style="text-align: right;">-0.114</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.793</td>
<td style="text-align: right;">17.146</td>
</tr>
<tr class="even">
<td style="text-align: left;">G2</td>
<td style="text-align: left;">x</td>
<td style="text-align: right;">3.318</td>
<td style="text-align: right;">3.071</td>
<td style="text-align: right;">0.9</td>
<td style="text-align: right;">0.247</td>
<td style="text-align: right;">7.326</td>
<td style="text-align: right;">2.836</td>
<td style="text-align: right;">3.071</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">-0.236</td>
<td style="text-align: right;">5.130</td>
</tr>
<tr class="odd">
<td style="text-align: left;">G2</td>
<td style="text-align: left;">z</td>
<td style="text-align: right;">1.012</td>
<td style="text-align: right;">1.017</td>
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">-0.005</td>
<td style="text-align: right;">17.524</td>
<td style="text-align: right;">1.257</td>
<td style="text-align: right;">1.017</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.240</td>
<td style="text-align: right;">11.580</td>
</tr>
<tr class="even">
<td style="text-align: left;">G2</td>
<td style="text-align: left;">I(z^2)</td>
<td style="text-align: right;">0.734</td>
<td style="text-align: right;">0.848</td>
<td style="text-align: right;">0.9</td>
<td style="text-align: right;">-0.114</td>
<td style="text-align: right;">5.799</td>
<td style="text-align: right;">0.691</td>
<td style="text-align: right;">0.848</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">-0.156</td>
<td style="text-align: right;">3.374</td>
</tr>
<tr class="odd">
<td style="text-align: left;">G2</td>
<td style="text-align: left;">x:z</td>
<td style="text-align: right;">3.051</td>
<td style="text-align: right;">3.046</td>
<td style="text-align: right;">0.8</td>
<td style="text-align: right;">0.005</td>
<td style="text-align: right;">6.165</td>
<td style="text-align: right;">3.111</td>
<td style="text-align: right;">3.046</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.064</td>
<td style="text-align: right;">3.865</td>
</tr>
<tr class="even">
<td style="text-align: left;">G3</td>
<td style="text-align: left;">(Intercept)</td>
<td style="text-align: right;">-1.295</td>
<td style="text-align: right;">-0.114</td>
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">-1.181</td>
<td style="text-align: right;">26.435</td>
<td style="text-align: right;">-0.012</td>
<td style="text-align: right;">-0.114</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.103</td>
<td style="text-align: right;">17.777</td>
</tr>
<tr class="odd">
<td style="text-align: left;">G3</td>
<td style="text-align: left;">x</td>
<td style="text-align: right;">3.711</td>
<td style="text-align: right;">3.071</td>
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">0.640</td>
<td style="text-align: right;">8.792</td>
<td style="text-align: right;">3.161</td>
<td style="text-align: right;">3.071</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.090</td>
<td style="text-align: right;">5.346</td>
</tr>
<tr class="even">
<td style="text-align: left;">G3</td>
<td style="text-align: left;">z</td>
<td style="text-align: right;">1.415</td>
<td style="text-align: right;">1.017</td>
<td style="text-align: right;">0.9</td>
<td style="text-align: right;">0.398</td>
<td style="text-align: right;">21.215</td>
<td style="text-align: right;">1.509</td>
<td style="text-align: right;">1.017</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.493</td>
<td style="text-align: right;">13.270</td>
</tr>
<tr class="odd">
<td style="text-align: left;">G3</td>
<td style="text-align: left;">I(z^2)</td>
<td style="text-align: right;">-0.102</td>
<td style="text-align: right;">0.848</td>
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">-0.950</td>
<td style="text-align: right;">6.230</td>
<td style="text-align: right;">0.333</td>
<td style="text-align: right;">0.848</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">-0.515</td>
<td style="text-align: right;">3.683</td>
</tr>
<tr class="even">
<td style="text-align: left;">G3</td>
<td style="text-align: left;">x:z</td>
<td style="text-align: right;">3.052</td>
<td style="text-align: right;">3.046</td>
<td style="text-align: right;">0.9</td>
<td style="text-align: right;">0.006</td>
<td style="text-align: right;">7.620</td>
<td style="text-align: right;">3.064</td>
<td style="text-align: right;">3.046</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.018</td>
<td style="text-align: right;">4.461</td>
</tr>
<tr class="odd">
<td style="text-align: left;">G4</td>
<td style="text-align: left;">(Intercept)</td>
<td style="text-align: right;">0.591</td>
<td style="text-align: right;">-0.114</td>
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">0.705</td>
<td style="text-align: right;">20.054</td>
<td style="text-align: right;">0.127</td>
<td style="text-align: right;">-0.114</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.242</td>
<td style="text-align: right;">17.615</td>
</tr>
<tr class="even">
<td style="text-align: left;">G4</td>
<td style="text-align: left;">x</td>
<td style="text-align: right;">3.006</td>
<td style="text-align: right;">3.071</td>
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">-0.065</td>
<td style="text-align: right;">6.046</td>
<td style="text-align: right;">3.058</td>
<td style="text-align: right;">3.071</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">-0.013</td>
<td style="text-align: right;">5.261</td>
</tr>
<tr class="odd">
<td style="text-align: left;">G4</td>
<td style="text-align: left;">z</td>
<td style="text-align: right;">0.506</td>
<td style="text-align: right;">1.017</td>
<td style="text-align: right;">0.9</td>
<td style="text-align: right;">-0.511</td>
<td style="text-align: right;">18.043</td>
<td style="text-align: right;">1.015</td>
<td style="text-align: right;">1.017</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">-0.002</td>
<td style="text-align: right;">12.243</td>
</tr>
<tr class="even">
<td style="text-align: left;">G4</td>
<td style="text-align: left;">I(z^2)</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.848</td>
<td style="text-align: right;">0.9</td>
<td style="text-align: right;">-0.848</td>
<td style="text-align: right;">4.940</td>
<td style="text-align: right;">0.423</td>
<td style="text-align: right;">0.848</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">-0.425</td>
<td style="text-align: right;">3.482</td>
</tr>
<tr class="odd">
<td style="text-align: left;">G4</td>
<td style="text-align: left;">x:z</td>
<td style="text-align: right;">3.427</td>
<td style="text-align: right;">3.046</td>
<td style="text-align: right;">0.9</td>
<td style="text-align: right;">0.381</td>
<td style="text-align: right;">5.823</td>
<td style="text-align: right;">3.216</td>
<td style="text-align: right;">3.046</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.170</td>
<td style="text-align: right;">4.103</td>
</tr>
<tr class="even">
<td style="text-align: left;">G4_eta</td>
<td style="text-align: left;">(Intercept)</td>
<td style="text-align: right;">0.654</td>
<td style="text-align: right;">-0.114</td>
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">0.768</td>
<td style="text-align: right;">22.943</td>
<td style="text-align: right;">0.387</td>
<td style="text-align: right;">-0.114</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.501</td>
<td style="text-align: right;">17.649</td>
</tr>
<tr class="odd">
<td style="text-align: left;">G4_eta</td>
<td style="text-align: left;">x</td>
<td style="text-align: right;">2.925</td>
<td style="text-align: right;">3.071</td>
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">-0.146</td>
<td style="text-align: right;">7.028</td>
<td style="text-align: right;">3.002</td>
<td style="text-align: right;">3.071</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">-0.069</td>
<td style="text-align: right;">5.263</td>
</tr>
<tr class="even">
<td style="text-align: left;">G4_eta</td>
<td style="text-align: left;">z</td>
<td style="text-align: right;">0.638</td>
<td style="text-align: right;">1.017</td>
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">-0.378</td>
<td style="text-align: right;">18.813</td>
<td style="text-align: right;">1.267</td>
<td style="text-align: right;">1.017</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.250</td>
<td style="text-align: right;">11.862</td>
</tr>
<tr class="odd">
<td style="text-align: left;">G4_eta</td>
<td style="text-align: left;">I(z^2)</td>
<td style="text-align: right;">0.287</td>
<td style="text-align: right;">0.848</td>
<td style="text-align: right;">0.9</td>
<td style="text-align: right;">-0.561</td>
<td style="text-align: right;">5.594</td>
<td style="text-align: right;">0.384</td>
<td style="text-align: right;">0.848</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">-0.464</td>
<td style="text-align: right;">3.613</td>
</tr>
<tr class="even">
<td style="text-align: left;">G4_eta</td>
<td style="text-align: left;">x:z</td>
<td style="text-align: right;">3.355</td>
<td style="text-align: right;">3.046</td>
<td style="text-align: right;">0.9</td>
<td style="text-align: right;">0.309</td>
<td style="text-align: right;">5.963</td>
<td style="text-align: right;">3.152</td>
<td style="text-align: right;">3.046</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.106</td>
<td style="text-align: right;">4.107</td>
</tr>
<tr class="odd">
<td style="text-align: left;">G1_G2_together</td>
<td style="text-align: left;">(Intercept)</td>
<td style="text-align: right;">1.041</td>
<td style="text-align: right;">-0.114</td>
<td style="text-align: right;">0.9</td>
<td style="text-align: right;">1.155</td>
<td style="text-align: right;">21.099</td>
<td style="text-align: right;">0.546</td>
<td style="text-align: right;">-0.114</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.661</td>
<td style="text-align: right;">17.495</td>
</tr>
<tr class="even">
<td style="text-align: left;">G1_G2_together</td>
<td style="text-align: left;">x</td>
<td style="text-align: right;">2.636</td>
<td style="text-align: right;">3.071</td>
<td style="text-align: right;">0.9</td>
<td style="text-align: right;">-0.435</td>
<td style="text-align: right;">6.407</td>
<td style="text-align: right;">2.868</td>
<td style="text-align: right;">3.071</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">-0.204</td>
<td style="text-align: right;">5.217</td>
</tr>
<tr class="odd">
<td style="text-align: left;">G1_G2_together</td>
<td style="text-align: left;">z</td>
<td style="text-align: right;">-0.490</td>
<td style="text-align: right;">1.017</td>
<td style="text-align: right;">0.9</td>
<td style="text-align: right;">-1.507</td>
<td style="text-align: right;">15.201</td>
<td style="text-align: right;">1.406</td>
<td style="text-align: right;">1.017</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.389</td>
<td style="text-align: right;">12.103</td>
</tr>
<tr class="even">
<td style="text-align: left;">G1_G2_together</td>
<td style="text-align: left;">I(z^2)</td>
<td style="text-align: right;">0.356</td>
<td style="text-align: right;">0.848</td>
<td style="text-align: right;">0.9</td>
<td style="text-align: right;">-0.492</td>
<td style="text-align: right;">4.524</td>
<td style="text-align: right;">0.731</td>
<td style="text-align: right;">0.848</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">-0.117</td>
<td style="text-align: right;">3.556</td>
</tr>
<tr class="odd">
<td style="text-align: left;">G1_G2_together</td>
<td style="text-align: left;">x:z</td>
<td style="text-align: right;">3.730</td>
<td style="text-align: right;">3.046</td>
<td style="text-align: right;">0.8</td>
<td style="text-align: right;">0.684</td>
<td style="text-align: right;">5.108</td>
<td style="text-align: right;">3.056</td>
<td style="text-align: right;">3.046</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.010</td>
<td style="text-align: right;">4.170</td>
</tr>
<tr class="even">
<td style="text-align: left;">G1_G3_together</td>
<td style="text-align: left;">(Intercept)</td>
<td style="text-align: right;">1.505</td>
<td style="text-align: right;">-0.114</td>
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">1.619</td>
<td style="text-align: right;">25.769</td>
<td style="text-align: right;">-0.090</td>
<td style="text-align: right;">-0.114</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.024</td>
<td style="text-align: right;">18.564</td>
</tr>
<tr class="odd">
<td style="text-align: left;">G1_G3_together</td>
<td style="text-align: left;">x</td>
<td style="text-align: right;">2.925</td>
<td style="text-align: right;">3.071</td>
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">-0.146</td>
<td style="text-align: right;">8.192</td>
<td style="text-align: right;">3.230</td>
<td style="text-align: right;">3.071</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.159</td>
<td style="text-align: right;">5.644</td>
</tr>
<tr class="even">
<td style="text-align: left;">G1_G3_together</td>
<td style="text-align: left;">z</td>
<td style="text-align: right;">2.467</td>
<td style="text-align: right;">1.017</td>
<td style="text-align: right;">0.9</td>
<td style="text-align: right;">1.450</td>
<td style="text-align: right;">19.392</td>
<td style="text-align: right;">2.937</td>
<td style="text-align: right;">1.017</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">1.920</td>
<td style="text-align: right;">13.599</td>
</tr>
<tr class="odd">
<td style="text-align: left;">G1_G3_together</td>
<td style="text-align: left;">I(z^2)</td>
<td style="text-align: right;">-0.378</td>
<td style="text-align: right;">0.848</td>
<td style="text-align: right;">0.8</td>
<td style="text-align: right;">-1.225</td>
<td style="text-align: right;">5.444</td>
<td style="text-align: right;">0.402</td>
<td style="text-align: right;">0.848</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">-0.446</td>
<td style="text-align: right;">3.810</td>
</tr>
<tr class="even">
<td style="text-align: left;">G1_G3_together</td>
<td style="text-align: left;">x:z</td>
<td style="text-align: right;">3.033</td>
<td style="text-align: right;">3.046</td>
<td style="text-align: right;">0.9</td>
<td style="text-align: right;">-0.013</td>
<td style="text-align: right;">6.684</td>
<td style="text-align: right;">2.640</td>
<td style="text-align: right;">3.046</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">-0.407</td>
<td style="text-align: right;">4.736</td>
</tr>
<tr class="odd">
<td style="text-align: left;">all_together</td>
<td style="text-align: left;">(Intercept)</td>
<td style="text-align: right;">-0.118</td>
<td style="text-align: right;">-0.114</td>
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">-0.004</td>
<td style="text-align: right;">21.933</td>
<td style="text-align: right;">0.182</td>
<td style="text-align: right;">-0.114</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.296</td>
<td style="text-align: right;">17.933</td>
</tr>
<tr class="even">
<td style="text-align: left;">all_together</td>
<td style="text-align: left;">x</td>
<td style="text-align: right;">3.205</td>
<td style="text-align: right;">3.071</td>
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">0.133</td>
<td style="text-align: right;">6.785</td>
<td style="text-align: right;">3.077</td>
<td style="text-align: right;">3.071</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.005</td>
<td style="text-align: right;">5.384</td>
</tr>
<tr class="odd">
<td style="text-align: left;">all_together</td>
<td style="text-align: left;">z</td>
<td style="text-align: right;">-0.784</td>
<td style="text-align: right;">1.017</td>
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">-1.801</td>
<td style="text-align: right;">17.498</td>
<td style="text-align: right;">1.547</td>
<td style="text-align: right;">1.017</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.530</td>
<td style="text-align: right;">12.026</td>
</tr>
<tr class="even">
<td style="text-align: left;">all_together</td>
<td style="text-align: left;">I(z^2)</td>
<td style="text-align: right;">-0.066</td>
<td style="text-align: right;">0.848</td>
<td style="text-align: right;">0.9</td>
<td style="text-align: right;">-0.913</td>
<td style="text-align: right;">5.100</td>
<td style="text-align: right;">0.486</td>
<td style="text-align: right;">0.848</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">-0.362</td>
<td style="text-align: right;">3.843</td>
</tr>
<tr class="odd">
<td style="text-align: left;">all_together</td>
<td style="text-align: left;">x:z</td>
<td style="text-align: right;">3.755</td>
<td style="text-align: right;">3.046</td>
<td style="text-align: right;">0.9</td>
<td style="text-align: right;">0.708</td>
<td style="text-align: right;">5.871</td>
<td style="text-align: right;">3.051</td>
<td style="text-align: right;">3.046</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.005</td>
<td style="text-align: right;">4.051</td>
</tr>
</tbody>
</table>


</div>
</div>
</div>
</section>
<section id="computational-time-for-hyperparameter-tuning" class="level3">
<h3 class="anchored" data-anchor-id="computational-time-for-hyperparameter-tuning">Computational time for hyperparameter tuning</h3>
<p>We evaluated the difference in computational time required for each group of hyperparameter tuning. As expected, we found that running XGBoost with default parameters takes the least amount of time (Fig 3). As we increase the number of parameters, the process becomes more computationally expensive. Moreover, sequential parameter tuning (e.g., first tuning the G1 group and then, in the next cycle, fixing G1 parameters and optimizing G2 parameters) takes significantly less time compared to tuning the same set of parameters simultaneously (G1 and G2 together) (Fig 3). Consistently, we observed that the mean computational time required for the 60% missing dataset is lower than that for the 20% missing dataset. This might be due to the fact that only complete cases are used for hyperparameter tuning, and the 60% missing dataset has fewer complete cases comparatively (Fig 3).</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="FigsAndTables/Hyperparameter_tuning_time_1.png" class="img-fluid figure-img" width="3000"></p>
<figcaption>Figure 3: Computational time analysis for hyperparameter tuning. Time was recorded in seconds and converted to a logarithmic scale for better visualization. All analyses were conducted with multi-threading enabled, utilizing six cores on a Mac computer.</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="simulation-results" class="level1">
<h1>Simulation results</h1>
<p>In this simulation study, we included all datasets (100 simulated datasets, as described in the study design) for each of the 20%, 40%, 60%, and 80% missingness scenarios. We compared various state-of-the-art imputation models, including <code>mice-PMM</code>, <code>mice-CART</code>, and <code>mice-RF</code>, as well as a recently developed XGBoost-based imputation algorithm (<code>mixgb</code> ) with multiple configurations of the <code>mice-xgb</code> algorithm. The <code>mice-xgb</code> algorithm was run with three different parameter configurations, which included running the algorithm with default parameters, tuned parameters for a randomly selected variable, and tuned parameters for all variables. Each configuration was executed with three different match types (match.type = predicted, predicted.observed, original.observed). Hence, in total 9 different configrations of <code>mice-xgb</code> were evaluated. Additionally, we performed all <code>mixgb</code> and <code>mice-xgb</code> imputations with both single iteration (maxit = 1) and multiple iterations (maxit = 5) to evaluate the impact of iterative imputation when using machine learning for imputation. For the parameter tuning configurations of <code>mice-xgb</code>, we employed sequential tuning of the G2 parameters, as this approach generally resulted in reduced bias compared to the default parameters, particularly for the 20% missing data scenario.</p>
<p>We observed that XGBoost model although showed increased coverage in maxit 1, system is getting more conerged with five iterations (Supplementary table) thus we will focus on maxit = 5 results in this section.</p>
<p>We first evaluated the performance of various imputation models on datasets with 20% missing data. Our results (Fig. 4) indicated that the imputation models <code>mice-PMM</code>, <code>mice-CART</code>, <code>mice-RF</code>, <code>mixgb</code> (with pmm 1 and 2), and the <code>mice-xgb</code> configurations (AllParam_predicted.observed, ParamDefault_predicted.observed) achieved 100% coverage for all terms, while <code>mice-xgb</code> with Random_predicted.observed achieved 99% coverage for all the terms. Other imputation models, including <code>mixgb</code> (no pmm) and <code>mice-xgb</code> (predicted and original.observed for all three parameter configurations), showed comparatively lower coverage. Additionally, CART and <code>mice-xgb</code>:Random_predicted.observed (coverage: 0.99) exhibited very similar and minimal bias for all terms except for the term <span class="math inline">\(Z^2\)</span>, where <code>mice-PMM</code> demonstrated less bias. Between the two methods, CART had a narrower width. The broader width observed in <code>mice-xgb</code>:Random_predicted.observed is expected, as it results from the additional variability introduced by tuning the subsampling parameter within the G2 group. <code>mice-RF</code> displayed slightly more bias than CART and Random_predicted.observed but with increased width. <code>mixgb</code> (with pmm) and ParamDefault_predicted.observed produced very similar results; although these three methods showed slightly more bias compared to CART and RF, Random_predicted.observed, they had the narrowest width. These findings underscored that when dealing with lower proportions of missing data, it may not be essential to tune all variables comprehensively. Instead, randomly tuning a single parameter can yield improved imputation results (Supplementary table).</p>
<p>When the missingness percentage was increased to 40%, significant differences in coverage among various imputation methods were observed (Fig 5). <code>mice-RF</code> consistently achieved the highest coverage with a perfect score of 1 across all terms evaluated. Similarly, <code>mice-CART</code>, <code>mice-PMM</code>, and <code>mice-xgb</code> using the <code>AllParam_predicted.observed</code> configuration also showed excellent coverage, ranging from 0.99 to 1. <code>mice-xgb</code> with <code>Random_predicted.observed</code> and <code>ParamDefault_predicted.observed</code> configurations exhibited slightly lower but still strong coverage rates, ranging between 0.96 and 1. In contrast, <code>Mixgb-PMM2</code> demonstrated better coverage (0.95 to 0.99) compared to Mixgb-PMM1 (0.94 to 1). However, other methods, including <code>mixgb</code> without PMM and <code>mice-xgb</code> with match.type set to predicted and original.observed for all parameters configurations, consistently resulted in poorer coverage outcomes. Next, we evaluated the bias and width of the models that showed more than 95% coverage. Among these methods, we observed that <code>mice-CART</code> and <code>mice-xgb</code>:ParamDefault_predicted.observed exhibited the least bias in the intercept and <span class="math inline">\(x\)</span>, both with a coverage of 1. However, for the <span class="math inline">\(z^2\)</span> term, <code>mice-PMM</code> (coverage: 1) demonstrated very small bias, alongside <code>mice-CART</code> (coverage: 0.99) and <code>mice-xgb</code>:ParamDefault_predicted.observed (coverage: 0.98). For the <span class="math inline">\(x:z\)</span> term, <code>mice-RF</code> (coverage: 1), <code>mice-CART</code> (coverage: 0.99), and ParamDefault_predicted.observed (coverage: 0.98) exhibited the lowest bias. For the <span class="math inline">\(z\)</span> term, <code>mice-xgb</code>:<code>ParamDefault_predicted.observed</code> (coverage: 0.96) and <code>mice-RF</code> (coverage: 1) showed very little bias. Although AllParam_predicted.observed (coverage: 1) also showed the least bias in z term, it had the widest width. In all five terms, <code>mice-xgb</code>:ParamDefault_predicted.observed had slightly more bias than CART/RF, but it consistently exhibited narrower width compared to the other top-performing methods. Furthermore, <code>mixgb</code> with pmm always produced the narrowest width compared to all methods, but this was often accompanied by reduced coverage and increased bias. Overall, we observed that <code>mice-xgb</code>:predicted.observed strikes a balance between these two strategies, with coverage and width between those of <code>mice-CART</code>/RF and mixgb-pmm. Moreover, these findings also suggest that it may not always be necessary to tune hyperparameters, as default parameters can sometimes yield better imputation results (Fig 5).</p>
<p>When the missingness percentage was increased to 60%, we observed greater variability in coverage plots among different imputation methods (Fig 5). State-of-the-art methods such as <code>mice-PMM</code>, CART, and RF consistently achieved coverage rates exceeding 95%. Specifically, <code>mice-RF</code> showed coverage between 0.98 and 0.99, CART between 0.96 and 0.99, and <code>mice-PMM</code> between 0.95 and 1. In contrast, <code>mice-xgb</code> with <code>ParamDefault_predicted.observed</code>, <code>AllParam_predicted.observed</code>, and <code>Random_predicted.observed</code> exhibited coverage ranging from 0.93 to 0.97, 0.91 to 0.95, and 0.92 to 0.96, respectively. <code>mixgb</code> demonstrated coverage of 0.84 to 0.91 in pmm-1 configurations and 0.89 to 0.96 in pmm-2 configurations. However, other configurations of <code>mice-xgb</code> (with match.type set to predicted and <code>original.observed</code>) and <code>mixgb</code> without PMM showed notably lower coverage between 0.41 and 0.75 (Fig 5).</p>
<p>Among the imputation methods achieving more than 95% coverage, we observed that out of 75 observations (9 configurations for <code>mice-xgb</code>, 3 configurations for mixgb-pmm, and 3 configurations for other mice methods, totaling 15 configurations multiplied by 5 terms), only 19 observations met this criterion consistently. <code>mice-PMM</code>, CART, and RF consistently demonstrated high coverage across all five terms evaluated. However, for the <span class="math inline">\(Z^2\)</span> term, mixgb-pmm2, ParamDefault_predicted.observed, and AllParam_predicted.observed also showed high coverage (&gt;95%), albeit with higher bias compared to CART/RF and wider intervals. Similarly, for the <span class="math inline">\(Z\)</span> term, <code>mice-xgb</code> with Random_predicted.observed achieved high coverage, yet exhibited bias levels between those of CART and RF, alongside broader intervals (Fig 6).</p>
<p>When the missingness was increased to 80%, we observed a decline in coverage across all methods. However, CART (0.97 - 0.99), RF (0.92 - 0.99), and <code>mice-PMM</code> (0.89 - 0.99) still exhibited the highest coverage compared to XGBoost models (Fig 7). Notably, <code>mice-xgb</code> with match.type = “predicted.observed” consistently showed better coverage than other match types, with coverage ranging between 0.68 and 0.94 for the 80% missingness scenarios. <code>mixgb</code> models with PMM showed coverage of 0.62 to 0.87 for PMM2 and 0.53 to 0.74 for PMM1. In contrast, <code>mixgb</code> without PMM and <code>mice-xgb</code> with match.type = “predicted” and original.observed displayed even lower coverage rates. Overall, the simulation results indicated superior performance of existing tree-based imputation methods compared to the XGBoost algorithm with default settings. However, the performance of XGBoost with tuned parameters was notably poorer, which aligns with the inconsistent results observed during the parameter tuning phase.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="FigsAndTables/Coverage_20p_iter5.png" class="img-fluid figure-img" width="1800"></p>
<figcaption>Figure 4: Comparison of Imputation Performance</figcaption>
</figure>
</div>
</div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="FigsAndTables/Bias_20p_iter5.png" class="img-fluid figure-img" width="1800"></p>
<figcaption>Figure 4: Comparison of Imputation Performance</figcaption>
</figure>
</div>
</div>
</div>
<p><em>Figure 4: Simulation results for 20% missingness. Imputation method showing coverage near to 1 and bias close to 0 indicate optimal results. The term “Random” denotes random selection of a variable as the target for hyperparameter tuning. “Allparam” signifies sequential tuning of hyperparameters across all variables. Within the XGBoost algorithm, “Predicted,” “predicted.observed,” and “original.observed” represent different match.type configurations used for integrating prediction and observation data</em></p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="FigsAndTables/Coverage_40p_iter5.png" class="img-fluid figure-img" width="1800"></p>
<figcaption>Figure 5: Comparison of Imputation Performance</figcaption>
</figure>
</div>
</div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="FigsAndTables/Bias_40p_iter5.png" class="img-fluid figure-img" width="1800"></p>
<figcaption>Figure 5: Comparison of Imputation Performance</figcaption>
</figure>
</div>
</div>
</div>
<p><em>Figure 5: Simulation results for 40% missingness. Imputation method showing coverage near to 1 and bias close to 0 indicate optimal results. The term “Random” denotes random selection of a variable as the target for hyperparameter tuning. “Allparam” signifies sequential tuning of hyperparameters across all variables. Within the XGBoost algorithm, “Predicted,” “predicted.observed,” and “original.observed” represent different match.type configurations used for integrating prediction and observation data</em></p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="FigsAndTables/Coverage_60p_iter5.png" class="img-fluid figure-img" width="1800"></p>
<figcaption>Figure 6: Comparison of Imputation Performance</figcaption>
</figure>
</div>
</div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="FigsAndTables/Bias_60p_iter5.png" class="img-fluid figure-img" width="1800"></p>
<figcaption>Figure 6: Comparison of Imputation Performance</figcaption>
</figure>
</div>
</div>
</div>
<p><em>Figure 6: Simulation results for 60% missingness. Imputation method showing coverage near to 1 and bias close to 0 indicate optimal results. The term “Random” denotes random selection of a variable as the target for hyperparameter tuning. “Allparam” signifies sequential tuning of hyperparameters across all variables. Within the XGBoost algorithm, “Predicted,” “predicted.observed,” and “original.observed” represent different match.type configurations used for integrating prediction and observation data</em></p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="FigsAndTables/Coverage_80p_iter5.png" class="img-fluid figure-img" width="1800"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="FigsAndTables/Bias_80p_iter5.png" class="img-fluid figure-img" width="1800"></p>
</figure>
</div>
</div>
</div>
<p><em>Figure 7: Simulation results for 80% missingness. Imputation method showing coverage near to 1 and bias close to 0 indicate optimal results. The term Random denotes random selection of a variable as the target for hyperparameter tuning. Allparam signifies sequential tuning of hyperparameters across all variables. Within the XGBoost algorithm, Predicted, predicted.observed, and original.observed represent different match.type configurations used for integrating prediction and observation data</em></p>
<section id="computational-time-required-for-imputation" class="level2">
<h2 class="anchored" data-anchor-id="computational-time-required-for-imputation">Computational time required for imputation</h2>
<p>We investigated the computational time required for various imputation methods. The future package in R was used to enable multi-threading, and all analyses were conducted utilizing six cores of the computer. For <code>mice-xgb</code> methods, the time taken for parameter tuning was included in the total imputation time before plotting. The results, presented in Figure 8 and Table 3, indicate that among the existing <code>mice</code> methods, <code>mice-PMM</code> (20% missingness: 0.10 seconds) required the least time, while <code>mice-RF</code> (20% missingness: 1.007 seconds) required the most time. However, the difference in imputation time between these methods was minimal. Implementations using XGBoost with default parameters in both <code>mice-xgb</code> and <code>mixgb</code> took longer than existing tree-based methods. Interestingly, <code>mice-xgb</code> with default settings (20% missingness: 1.87 - 2.29 seconds) was faster compared to <code>mixgb</code> (20% missingness: 2.73 - 3.62 seconds), likely due to subsampling used in <code>mixgb</code> (Fig 8; Table 3).</p>
<p>Parameter tuning significantly increased computational time relative to all other imputation strategies. Tuning a single parameter was less computationally intensive (20% missingness: 37.20 - 37.44 seconds) compared to tuning all parameters (20% missingness: 62.27 - 64.97 seconds). There was a negligible difference in computational time across varying proportions of missingness, and in some cases, higher missingness slightly reduced the computational time.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="FigsAndTables/Imputation_time.png" class="img-fluid figure-img" width="1500"></p>
<figcaption>Figure 8: Computational time for all imputation methods. All imputation algorithms were run with maxit = 5 using mutli-threading option (6 cores)</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div class="tabwid"><style>.cl-54446c0c{}.cl-54410774{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-54426d76{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-54426d80{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-544276f4{width:1.3in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-544276f5{width:1.3in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-544276fe{width:1.3in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-544276ff{width:1.3in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-54427708{width:1.3in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-54427709{width:1.3in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table data-quarto-disable-processing="true" class="cl-54446c0c"><thead><tr style="overflow-wrap:break-word;"><th class="cl-544276f4"><p class="cl-54426d76"><span class="cl-54410774">Methods</span></p></th><th class="cl-544276f5"><p class="cl-54426d80"><span class="cl-54410774">20% missingness</span></p></th><th class="cl-544276f5"><p class="cl-54426d80"><span class="cl-54410774">40% missingness</span></p></th><th class="cl-544276f5"><p class="cl-54426d80"><span class="cl-54410774">60% missingness</span></p></th><th class="cl-544276f5"><p class="cl-54426d80"><span class="cl-54410774">80% missingness</span></p></th></tr></thead><tbody><tr style="overflow-wrap:break-word;"><td class="cl-544276fe"><p class="cl-54426d76"><span class="cl-54410774">PMM</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">0.10120</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">0.10137</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">0.10135</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">0.10230</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-544276fe"><p class="cl-54426d76"><span class="cl-54410774">CART</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">0.60204</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">0.62746</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">0.64086</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">0.63955</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-544276fe"><p class="cl-54426d76"><span class="cl-54410774">RF</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">1.00735</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">1.01215</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">0.98927</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">0.99465</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-544276fe"><p class="cl-54426d76"><span class="cl-54410774">ParamDefault_predicted</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">1.86751</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">1.97423</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">2.04909</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">2.05634</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-544276fe"><p class="cl-54426d76"><span class="cl-54410774">ParamDefault_predicted.observed</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">2.06359</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">2.11853</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">2.10129</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">2.50600</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-544276fe"><p class="cl-54426d76"><span class="cl-54410774">ParamDefault_original_observed</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">2.29017</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">2.56071</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">2.54580</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">2.50515</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-544276fe"><p class="cl-54426d76"><span class="cl-54410774">Random_predicted</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">37.41400</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">31.75157</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">29.32893</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">28.45460</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-544276fe"><p class="cl-54426d76"><span class="cl-54410774">Random_predicted.observed</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">37.20259</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">31.51315</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">29.68158</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">28.78559</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-544276fe"><p class="cl-54426d76"><span class="cl-54410774">Random_original.observed</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">37.43501</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">32.01577</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">29.91800</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">25.92068</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-544276fe"><p class="cl-54426d76"><span class="cl-54410774">AllParam_predicted</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">64.96625</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">68.36165</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">65.58093</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">59.37096</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-544276fe"><p class="cl-54426d76"><span class="cl-54410774">AllParam_predicted.observed</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">64.64511</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">67.61957</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">65.66513</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">59.63626</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-544276fe"><p class="cl-54426d76"><span class="cl-54410774">AllParam_original.observed</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">62.26728</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">65.17236</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">63.54737</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">57.60734</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-544276fe"><p class="cl-54426d76"><span class="cl-54410774">Mixgb - Pmm.Null</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">2.72729</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">2.75106</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">2.77861</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">2.65826</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-544276fe"><p class="cl-54426d76"><span class="cl-54410774">Mixgb - Pmm.1</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">3.61647</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">3.51557</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">3.43545</span></p></td><td class="cl-544276ff"><p class="cl-54426d80"><span class="cl-54410774">3.48340</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-54427708"><p class="cl-54426d76"><span class="cl-54410774">Mixgb - Pmm.2</span></p></td><td class="cl-54427709"><p class="cl-54426d80"><span class="cl-54410774">3.44440</span></p></td><td class="cl-54427709"><p class="cl-54426d80"><span class="cl-54410774">3.23028</span></p></td><td class="cl-54427709"><p class="cl-54426d80"><span class="cl-54410774">2.99131</span></p></td><td class="cl-54427709"><p class="cl-54426d80"><span class="cl-54410774">2.96996</span></p></td></tr></tbody></table></div>
</div>
</div>
</section>
</section>
<section id="discussion" class="level1">
<h1>Discussion</h1>
<p>Traditional methods for imputing missing data often encounter challenges when the variables demonstrate non-linear relationships. To address this limitation, tree-based methods such as Classification and Regression Trees (CART) and Random Forests (RF) were introduced, which exhibited improved accuracy in capturing complex data relationships. In recent years, the XGBoost algorithm has gained widespread acceptance due to its superior predictive power and flexibility. Deng et al.&nbsp;previously leveraged XGBoost for imputation purposes and developed the mixgb package for this specific application. In our study, we integrated XGBoost into the iterative framework of Multiple Imputation by Chained Equations (MICE). Similar to mixgb, we integrated three distinct matching strategies with XGBoost predictions. Importantly, <code>mice-xgb</code> offers extensive parameter tuning capabilities, allowing customization for various datasets and modeling objectives.</p>
<p>Our comparative simulation analysis revealed that among the three matching strategies proposed, <code>mice-xgb</code> performed best with the “predicted.observed” match.type Conversely, the other two strategies exhibited markedly poorer performance, necessitating further investigation into their potential use under variety of different conditions. Specifically, simulation results indicated that <code>mice-xgb</code> with “predicted.observed” strategy achieved comparable results to CART and RF. Under default parameter settings, <code>mice-xgb</code> showed narrower confidence intervals compared to CART/RF with same or marginal decrease in coverage (1-2%). Our study positioned <code>mice-xgb</code>: predicted.observed performance between CART/RF and mixgb-pmm, with CART/RF demonstrating the widest confidence intervals, minimal bias, and high coverage. Transitioning to mixgb, we observed a slight increase in bias alongside reduced coverage. However, as missing data proportions increased, both <code>mice-xgb</code> and <code>mixgb</code> exhibited significantly biased estimates, whereas CART/RF continued to perform better, even with 80% missing data.</p>
<p>Our exploration of parameter-tuned configurations for <code>mice-xgb</code> highlighted poor performance of the model. However, parameter tuning is a complex and it was not possible to evaluate hyperparameters for each of the 100 simulated datasets with varying levels of missingness. Future studies should prioritize evaluating the importance of individual parameters under increased missingness scenarios and focus on their specific optimization before generalizing to wide variety of datasets. We observed that parameter-tuned XGBoost models displayed wider intervals compared to other models and mixgb. This increased variability can be attributed to three main factors: variations in initial imputation, subsampling parameters, and donor-based selection. In contrast, <code>mixgb</code> use consistent initial imputations across datasets and variability in <code>mixgb</code> originate from subsampling and donor-based selection, which resulted in narrower width and less coverage. Future research could investigate the performance of <code>mice-xgb</code> by excluding subsampling parameters from the list of tunable parameters. It is important to clarify that our study did not emphasize the importance of any specific set of parameters but rather aimed to demonstrate the application of parameter tuning features in <code>mice-xgb</code> imputation. Furthermore, tuning parameters for each variable in the dataset (Allparam) is a computationally intensive task. Moving forward, exploring multi-task parameter tuning methods could offer a promising avenue for future investigations.</p>
<p>Although, <code>mice-xgb</code> parameter tuned configurations also showed poor performance in our study design, parameter tuning is a extensive process and need to be optimized for each dataset which was not possible for 100 simulated datasets with varying percentages of missingness. It will be better to evaluate importance of each parameters for increased missingness scenario and specifically tune those parameter. Moreover, we have exployed mean absolute percentage error as a loss function in XGBoost algorit that is being minimized in <code>mice-xgb</code>. For future studies, it will be a good idea to evalue other loss functions more adaptive to missing data problems. Furthermore, our approach utilized mean absolute percentage error as a loss function minimized in <code>mice-xgb</code>. For future investigations, exploring alternative loss functions tailored to missing data challenges could provide valuable insights.</p>
<p>In summary, <code>mice-xgb</code> represents a promising approach for missing data imputation, particularly with the predicted.observed strategy, showing competitive performance relative to established methods like CART and RF. However, further exploration is warranted to optimize its performance across varying degrees of missingness and to refine its applicability in different analytical contexts.</p>

</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-buurenFlexibleImputationMissing" class="csl-entry" role="listitem">
Buuren, Stef van. n.d. <em>Flexible <span>Imputation</span> of <span>Missing</span> <span>Data</span></em>.
</div>
<div id="ref-dengMultipleImputationXGBoost2023" class="csl-entry" role="listitem">
Deng, Yongshi, and Thomas Lumley. 2023. <span>“Multiple <span>Imputation</span> <span>Through</span> <span>XGBoost</span>.”</span> <em>Journal of Computational and Graphical Statistics</em> 0 (0): 1–19. <a href="https://doi.org/10.1080/10618600.2023.2252501">https://doi.org/10.1080/10618600.2023.2252501</a>.
</div>
<div id="ref-Rubin_1994" class="csl-entry" role="listitem">
Efron, Bradley. 1994. <span>“Missing Data, Imputation, and the Bootstrap.”</span> <em>Journal of the American Statistical Association</em> 89 (426): 463–75. <a href="http://www.jstor.org/stable/2290846">http://www.jstor.org/stable/2290846</a>.
</div>
<div id="ref-kangPreventionHandlingMissing2013" class="csl-entry" role="listitem">
Kang, Hyun. 2013. <span>“The Prevention and Handling of the Missing Data.”</span> <em>Korean Journal of Anesthesiology</em> 64 (5): 402. <a href="https://doi.org/10.4097/kjae.2013.64.5.402">https://doi.org/10.4097/kjae.2013.64.5.402</a>.
</div>
<div id="ref-littleMissingDataAdjustmentsLarge1988" class="csl-entry" role="listitem">
Little, Roderick J. A. 1988. <span>“Missing-<span>Data</span> <span>Adjustments</span> in <span>Large</span> <span>Surveys</span>.”</span> <em>Journal of Business &amp; Economic Statistics</em> 6 (3): 287–96. <a href="https://doi.org/10.2307/1391878">https://doi.org/10.2307/1391878</a>.
</div>
<div id="ref-nijmanRealtimeImputationMissing2021" class="csl-entry" role="listitem">
Nijman, Steven W J, Jeroen Hoogland, T Katrien J Groenhof, Menno Brandjes, John J L Jacobs, Michiel L Bots, Folkert W Asselbergs, Karel G M Moons, Thomas P A Debray, and On behalf of the UCC-CVRM and UCC-SMART study groups. 2021. <span>“Real-Time Imputation of Missing Predictor Values in Clinical Practice.”</span> <em>European Heart Journal - Digital Health</em> 2 (1): 154–64. <a href="https://doi.org/10.1093/ehjdh/ztaa016">https://doi.org/10.1093/ehjdh/ztaa016</a>.
</div>
<div id="ref-obermanStandardizedEvaluationImputation2024" class="csl-entry" role="listitem">
Oberman, Hanne I., and Gerko Vink. 2024. <span>“Toward a Standardized Evaluation of Imputation Methodology.”</span> <em>Biometrical Journal</em> 66 (1): 2200107. <a href="https://doi.org/10.1002/bimj.202200107">https://doi.org/10.1002/bimj.202200107</a>.
</div>
<div id="ref-rubinInferenceMissingData1976" class="csl-entry" role="listitem">
RUBIN, DONALD B. 1976. <span>“Inference and Missing Data.”</span> <em>Biometrika</em> 63 (3): 581–92. <a href="https://doi.org/10.1093/biomet/63.3.581">https://doi.org/10.1093/biomet/63.3.581</a>.
</div>
<div id="ref-Rubin1987" class="csl-entry" role="listitem">
<span>“Rubin, Page 76.”</span> 1987. In <em>Multiple <span>Imputation</span> for <span>Nonresponse</span> in <span>Surveys</span></em>, 75–112. John Wiley &amp; Sons, Ltd. <a href="https://doi.org/10.1002/9780470316696.ch3">https://doi.org/10.1002/9780470316696.ch3</a>.
</div>
<div id="ref-schaferMissingDataOur2002" class="csl-entry" role="listitem">
Schafer, Joseph L., and John W. Graham. 2002. <span>“Missing Data: <span>Our</span> View of the State of the Art.”</span> <em>Psychological Methods</em> 7 (2): 147–77. <a href="https://doi.org/10.1037/1082-989X.7.2.147">https://doi.org/10.1037/1082-989X.7.2.147</a>.
</div>
<div id="ref-stekhovenMissForestNonparametricMissing2012" class="csl-entry" role="listitem">
Stekhoven, Daniel J., and Peter Bühlmann. 2012. <span>“<span>MissForest</span>—Non-Parametric Missing Value Imputation for Mixed-Type Data.”</span> <em>Bioinformatics</em> 28 (1): 112–18. <a href="https://doi.org/10.1093/bioinformatics/btr597">https://doi.org/10.1093/bioinformatics/btr597</a>.
</div>
<div id="ref-vanbuurenFullyConditionalSpecification2006" class="csl-entry" role="listitem">
Van Buuren, S., J. P. L. Brand, C. G. M. Groothuis-Oudshoorn, and D. B. Rubin. 2006. <span>“Fully Conditional Specification in Multivariate Imputation.”</span> <em>Journal of Statistical Computation and Simulation</em> 76 (12): 1049–64. <a href="https://doi.org/10.1080/10629360600810434">https://doi.org/10.1080/10629360600810434</a>.
</div>
<div id="ref-wulffMultipleImputationChained2017" class="csl-entry" role="listitem">
Wulff, Jesper N, and Linda Ejlskov. 2017. <span>“Multiple <span>Imputation</span> by <span>Chained</span> <span>Equations</span> in <span>Praxis</span>: <span>Guidelines</span> and <span>Review</span>”</span> 15 (1).
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></div></div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>